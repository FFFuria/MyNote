#Python #深度学习 #鱼书
# Python简入门
## Numpy
### 广播机制
NumPy 中,形状不同的数组之间也可以进行运算，如图所示,  标量 10 被扩展成了 2 × 2 的形状,然后再与矩阵 A 进行乘法运算
![[file-20251105202159126.png|385]]
### NumPy一维数组的特殊性
一维数组 (b,) 放在 **左边** 时，会被当成 (1, b)；  
放在 **右边** 时，会被当成 (b, 1)。  
官方叫 **broadcasting**，但记住“缺哪边补 1”就行。
# 感知机
## 相关定义
*深度学习中的神经元*
**神经元激活**：神经元会计算传送过来的信号的总和,只有当这个总和超过了某个界限值时,才会输出1
$y = \begin{cases} 0 & (w_1x_1 + w_2x_2 \leqslant \theta) \\ 1 & (w_1x_1 + w_2x_2 > \theta) \end{cases}$
## 数电相关
* 与门
* 与非门
* 或门
* 异或门
### 异或门的多层原因
#### 线性空间
感知机的局限性就在于它只能表示由一条直线分割的空间
对于与门、与非门以及或门，均可如图采用一条直线分隔开来
![[file-20251105202159121.png|446]]
#### 非线性空间
对于异或门，无法采用一条直线分割区域，需要如图采用曲线进行分割
![[file-20251105202159126 1.png|446]]
因此异或门的实现需要采用多层门的叠加 *（此处采用两层门）*
![[file-20251105202159128 1.png|446]]
# 神经网络
## 偏置参数
将感知机当中的阈值转换成 $b$ ，用是否大于0表示是否激活神经元，由此引入偏置参数 $b$
$y = \begin{cases} 0 & (b + w_1x_1 + w_2x_2 \leqslant 0) \\ 1 & (b + w_1x_1 + w_2x_2 > 0) \end{cases}$
$b$ 是被称为偏置的参数,用于控制神经元被激活的容易程度;而 $w1$ 和 $w2$  是表示各个信号的权重的参数,用于控制各个信号的重要性。
## 激活函数
*一般地,回  归问题可以使用恒等函数,二元分类问题可以使用 sigmoid 函数,  多元分类问题可以使用 softmax 函数。*
激活函数是在最终输出信号之前的函数，用于将输入信号的总和进行一定公式的转换输出
如下面函数中的 $h(a)$ 即属于激活函数的一种：
### 阶跃函数
$a = b + w_1x_1 + w_2x_2$ 
$y = h(a)$
阶跃函数是指一旦输入超过阈值,就切换输出的函数
### sigmoid函数
$h(x) = \frac{1}{1 + \exp(-x)}$
向 sigmoid 函数输入 1.0 或 2.0  后,就会有某个值被输出,类似 h(1.0) = 0.731 . . .、h(2.0) = 0.880 . . . 这样
### ReLU函数
ReLU函数在输入大于 0 时,直接输出该值;在输入小于等于 0 时,输出 0。
### softmax函数
$y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}$
#### 改进
![[file-20251105202159133.png|342]]
核心结论：**“在进行 softmax 的指数函数的运算时,加上(或者减去) 某个常数并不会改变运算的结果。**
我们想办法让**最大的指数尽可能小**。  
通过减去输入信号的最大值，将所有输入“拉低”到一个安全的、不会导致数值溢出的区间，从而解决了计算机在实现 Softmax 运算时的缺陷。
#### 被省略
*输出层的softmax一般会被省去，原因如下：*
**训练的目标**：训练的目标是**学习**。模型需要知道自己“错得有多离谱”，然后根据这个错误来调整参数（权重）。**需要损失函数**：为了量化“错得有多离谱”，我们需要一个损失函数（比如交叉熵损失函数）。**损失函数需要概率**：交叉熵损失函数的计算**必须**基于概率分布。它比较的是模型输出的概率分布和真实的标签分布之间的差距。原始的得分 【 3.0, 1.0, 0.2 】并不是概率，无法用来计算交叉熵损失。**Softmax提供概率**：Softmax的作用就是将这些原始得分转化成一个合法的、可微的概率分布 【0.836, 0.113, 0.051】，这样才能进行后续的损失计算和反向传播来更新模型。
### 非线性函数
阶跃函数和 sigmoid 函数两者均为 **非线性函数** 。sigmoid 函数是一条曲线,阶跃函数是一条像阶梯一样的折线
![[file-20251105202159128.png|612]]
**激活函数不能使用线性函数。**
线性函数的问题在于,不管如何加深层数,总是存在与之等效的“无隐藏层的神经网络”。
ex.考虑把线性函数 $h(x) = cx$ 作为激活函数,把 $y(x) = h(h(h(x)))$ 的运算对应3层神经网络。
但是同样的处理可以由 $y(x) = ax$
这一次乘法运算(即没有隐藏层的神经网络)来表示。
如本例所示,  使用线性函数时,无法发挥多层网络带来的优势
